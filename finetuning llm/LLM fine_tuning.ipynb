{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U accelerate==0.29.3\t# PyTorch 모델의 학습 속도 향상과 추론 최적화를 위한 라이브러리\n",
    "!pip install peft==0.10.0\t#대규모 언어 모델을 효율적으로 미세 조정할 수 있는 PEFT 기술 구현\n",
    "!pip install bitsandbytes==0.43.1\t# 모델 매개변수 양자화로 메모리 사용량 절감\n",
    "!pip install transformers==4.40.1\n",
    "!pip install trl==0.8.6\t# Transformer Reinforcement Learning의 약자로 강화 학습 기반 언어 모델 미세 조정 기술 구현\n",
    "!pip install datasets==2.19.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer\n",
    "\n",
    "import huggingface_hub\n",
    "huggingface_hub.login(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face Basic Model 한국어 모델\n",
    "base_model = \"beomi/Llama-3-Open-Ko-8B\"\t# beomi님의 Llama3 한국어 파인튜닝 모델\n",
    "\n",
    "# 보험문서 pdf json파일로 변환한거\n",
    "YTcode_dataset = \"/content/dataset\"\n",
    "\n",
    "# 새로운 모델 이름\n",
    "new_model = \"finetuning_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(YTcode_dataset, split=\"train\")\n",
    "\n",
    "# dataset = dataset.select(range(200))\n",
    "\n",
    "# 데이터 확인\n",
    "print(len(dataset))\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 현재 사용중인 GPU의 CUDA 연산 능력을 확인한다.\n",
    "# 8이상이면 고성능 GPU 로 판단한다.\n",
    "if torch.cuda.get_device_capability()[0] >= 8:\n",
    "    !pip install -qqq flash-attn\n",
    "    # 고성능 Attention인 flash attention 2 을 사용\n",
    "    attn_implementation = \"flash_attention_2\"\n",
    "    # 데이터 타입을 bfloat16으로 설정해준다.\n",
    "    # bfloat16은 메모리 사용량을 줄이면서도 계산의 정확성을 유지할 수 있는 데이터 타입이다.\n",
    "    torch_dtype = torch.bfloat16\n",
    "else:\n",
    "    attn_implementation = \"eager\"\n",
    "    torch_dtype = torch.float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QLoRA config\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\t# 모델 가중치를 4비트로 로드\n",
    "    bnb_4bit_quant_type=\"nf4\",\t# 양자화 유형으로는 “nf4”를 사용한다.\n",
    "    bnb_4bit_compute_dtype=torch_dtype,\t# 양자화를 위한 컴퓨팅 타입은 직전에 정의 했던 torch_dtype으로 지정 해준다.\n",
    "    bnb_4bit_use_double_quant=False,\t# 이중 양자화는 사용하지 않는다.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 로드\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=quant_config,\n",
    "    device_map={\"\": 0}\t# 0번째 gpu 에 할당\n",
    ")\n",
    "# 모델의 캐시 기능을 비활성화 한다. 캐시는 이전 계산 결과를 저장하기 때문에 추론 속도를 높이는 역할을 한다. 그러나 메모리 사용량을 증가시킬 수 있기 때문에, 메모리부족 문제가 발생하지 않도록 하기 위해 비활성화 해주는 것이 좋다.\n",
    "model.config.use_cache = False\n",
    "# 모델의 텐서 병렬화(Tensor Parallelism) 설정을 1로 지정한다. 설정값 1은 단일 GPU에서 실행되도록 설정 해주는 의미이다.\n",
    "model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "              base_model,\n",
    "              trust_remote_code=True)\n",
    "# 시퀀스 길이를 맞추기 위해 문장 끝에 eos_token를 사용\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# 패딩 토큰을 시퀀스의 어느 쪽에 추가할지 설정\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_params = LoraConfig(\n",
    "    lora_alpha=16,\t# LoRA의 스케일링 계수를 설정 한다. 값이 클 수록 학습 속도가 빨라질 수 있지만, 너무 크게 되면 모델이 불안정해질 수 있다.\n",
    "    lora_dropout=0.1,\t#  과적합을 방지하기 위한 드롭아웃 확률을 설정한다. 여기서는 10%(0.1)의 드롭아웃 확률을 사용하여 모델의 일반화 성능을 향상시킨다.\n",
    "    r=64,\t# LoRA 어댑터 행렬의 Rank를 나타낸다. 랭크가 높을수록 모델의 표현 능력은 향상되지만, 메모리 사용량과 학습 시간이 증가한다. 일반적으로 4, 8, 16, 32, 64 등의 값을 사용한다.\n",
    "    bias=\"none\",\t# LoRA 어댑터 행렬에 대한 편향을 추가할지 여부를 결정한다. “none”옵션을 사용하여 편향을 사용하지 않는다.\n",
    "    task_type=\"CAUSAL_LM\",\t# LoRA가 적용될 작업 유형을 설정한다. CAUSAL_LM은 Causal Language Modeling 작업을 의미한다. 이는 특히 GPT 같은 텍스트 생성 모델에 주로 사용된다.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# CUDA 메모리 캐시 비우기\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "training_params = TrainingArguments(\n",
    "    output_dir=\"/results\",\n",
    "    num_train_epochs=1,  # epoch는 1로 설정\n",
    "    max_steps=100,  # max_steps을 더 줄이기\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=2,  # 누적 배치 크기 줄이기\n",
    "    optim=\"paged_adamw_8bit\",  # 메모리 효율적인 optimizer 사용\n",
    "    warmup_steps=0,  # warmup steps를 0으로 설정\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,  # mixed precision training\n",
    "    logging_steps=100,\n",
    "    push_to_hub=False,\n",
    "    report_to='none',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화 함수\n",
    "def tokenize_function(examples):\n",
    "    # 'pages' 필드가 리스트로 되어있기 때문에 이를 문자열로 합침\n",
    "    combined_pages = \" \".join(examples['pages'])  # 리스트를 문자열로 결합\n",
    "    return tokenizer(combined_pages, padding=\"max_length\", truncation=True, max_length=256)\n",
    "\n",
    "# 데이터셋 토큰화\n",
    "tokenized_dataset = dataset.map(tokenize_function)\n",
    "\n",
    "# 파인튜닝용 Trainer 설정\n",
    "trainer = SFTTrainer(\n",
    "    model=model,                   # 미리 로드된 모델을 여기에 넣어야 함\n",
    "    train_dataset=tokenized_dataset,  # 토큰화된 데이터셋 사용\n",
    "    peft_config=peft_params,\n",
    "    dataset_text_field=\"pages\",        # 'pages' 필드를 학습에 사용\n",
    "    max_seq_length=256,              # 원하는 최대 시퀀스 길이\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_params,\n",
    "    packing=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 프롬프트 설정\n",
    "prompt = \"댕댕이 보험에 대한 상세 정보를 알려주세요\"\n",
    "\n",
    "# pipeline 설정 (모델과 토크나이저를 미리 정의)\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# 텍스트 생성\n",
    "result = pipe(\n",
    "    f\"<s>[INST] {prompt} [/INST]\",\n",
    "    max_length=200,              # 생성되는 텍스트의 최대 길이 설정\n",
    "    do_sample=True,              # 샘플링 방식 사용 (무작위성 도입)\n",
    "    temperature=0.7,             # 온도 설정 (값이 낮을수록 보수적인 결과)\n",
    "    top_k=50,                    # 상위 50개의 후보 토큰만 고려\n",
    "    top_p=0.9,                   # 상위 확률의 합이 0.9인 후보 토큰들만 고려 (nucleus sampling)\n",
    "    repetition_penalty=1.2        # 반복 억제 설정 (반복되는 토큰에 페널티)\n",
    ")\n",
    "\n",
    "# 결과 출력\n",
    "print(result[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
